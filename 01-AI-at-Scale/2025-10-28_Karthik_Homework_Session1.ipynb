{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc9fb72-302c-4b6e-944b-3af8dcbaa4bf",
   "metadata": {},
   "source": [
    "1. Make sure that the single GPU code runs on Polaris.\n",
    "- Yes. The script runs correctly on a single GPU, completing a 10-epoch sweep in 16.26 s. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7fe19-f677-49fd-8c80-b783bec46068",
   "metadata": {},
   "source": [
    "2. The counting of ranks, does not necessarily has to be a mix-and-match between mpi4py and PALS. Try to implement the rank counting method using just PALS or mpi4py. device_count() methods can be useful here.\n",
    "- We can use the following commands for a pure mpi4py implementation:\n",
    "```\n",
    "MPI-only: RANK=MPI.COMM_WORLD.Get_rank()\n",
    "WORLD_SIZE=MPI.COMM_WORLD.Get_size()\n",
    "LOCAL_RANK=RANK % torch.cuda.device_count()\n",
    "```\n",
    "and the following for a PALS only implementation:\n",
    "```\n",
    "RANK=int(os.environ[\"PALS_RANKID\"])\n",
    "WORLD_SIZE=int(os.environ[\"PALS_SIZE\"])\n",
    "LOCAL_RANK=int(os.environ[\"PALS_LOCAL_RANKID\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21e639-9114-44d9-905d-b6493dc5c513",
   "metadata": {},
   "source": [
    "3. Play with different dimensions of the src and tgt tensors.\n",
    "- Timing various target lengths with same source lengths shows smooth growth up to tgt=60, then a sharp jump for src=1 at tgt≥70. Below are some configurations I tried:\n",
    "```\n",
    "(1,20): 4.57 s;\n",
    "(1,1): 4.33 s;\n",
    "(1,50): 4.87 s;\n",
    "(1,60): 5.33 s;\n",
    "(60,1): 5.49 s;\n",
    "(60,60): 7.78 s;\n",
    "(1,70): 50.47 s;\n",
    "(1,75): 44.48 s;\n",
    "(1,100): 45.50 s.\n",
    "```\n",
    "![HW1](hw1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7776a-0228-48f5-a2f6-1b3eaba64fb5",
   "metadata": {},
   "source": [
    "4. Explore the cost of collective communication, by setting up a scenario, where you have only two ranks, but each rank resides on a different node. Profile and try to reason about the results.\n",
    "- With two ranks on the same node the run took ~11 s; placing one rank per node increased total time to ~38 s. Profiling shows nccl:all_reduce rising from ~8 ms intra-node to ~199 ms inter-node. The slowdown is communication-bound rather than compute-bound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb6821-99ba-4df2-860d-8ea2f67bb417",
   "metadata": {},
   "source": [
    "5. Try other file formats to explore the I/O bottleneck.\n",
    "- Replacing HDF5 with a single .pt file reduced average DataLoader step wall time from ~2.8 ms (HDF5) to ~0.96 ms (.pt). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6476031-5c21-4665-8805-7053df36be19",
   "metadata": {},
   "source": [
    "6. Make the tensors really large, specially the 2nd and 3rd dimension and explore different data types.\n",
    "- Continuing from the observations from Question 3, we notice that there is a sudden jump in increasing the sequence lengths exposing a data-pipeline bottleneck. Between tgt=60 and 70, total time inflated from 5.33 s to 50.47–63.90 s; profiling attributes the jump to CPU-side batch construction, where the default collate’s aten::stack path incurs heavy copies/allocations at that size, starving the GPU and reducing overlap with NCCL. I tried float16/bfloat16 but it didn't help with this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb41ea-c39f-43e6-985c-81bdf1f5f65f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2025-09-25",
   "language": "python",
   "name": "2025-09-25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
